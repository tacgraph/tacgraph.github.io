<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description"
          content="TacGraph: Utilizing contact priors to inform simultaneous pose and contact estimation.">
    <meta name="keywords" content="TacGraph, Tactile, Prehensile Manipulation">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>TacGraph</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/icon_full.jpg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">TacGraph</h1>
                    <h2 class="subtitle is-3 publication-subtitle">
                        Simultaneous Extrinsic Contact and In-Hand Pose Estimation via Distributed Tactile Sensing
                    </h2>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://mvandermerwe.github.io/">Mark Van der Merwe</a><sup>1</sup>,</span>
                        <span class="author-block">
              <a href="https://keiohta.github.io/">Kei Ota</a><sup>2</sup>,</span>
                        <span class="author-block">
              <a href="https://berenson.robotics.umich.edu/">Dmitry Berenson</a><sup>1</sup>,
            </span>
                        <span class="author-block">
              <a href="https://www.mmintlab.com/people/nima-fazeli/">Nima Fazeli</a><sup>1</sup>,
            </span>
                        <span class="author-block">
              <a href="https://www.linkedin.com/in/deveshjha/">Devesh Jha</a><sup>3</sup>
            </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>University of Michigan,</span>
                        <span class="author-block"><sup>2</sup>Mitsubishi Electric</span>
                        <span class="author-block"><sup>3</sup>Mitsubishi Electric Research Laboratories</span>
                    </div>

                    <div class="is-size-4 publication-authors">
                        <b>IEEE Robotics and Automation Letters 2026</b>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <span class="link-block">
                <a href="https://arxiv.org/abs/2512.23856"
                   target="_blank"
                   rel="noopener noreferrer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <video id="teaser" autoplay muted loop playsinline height="100%">
                <source src="./static/videos/tacgraph_overview.mp4"
                        type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
                <b>TacGraph</b> utilizes visual and distributed tactile feedback to estimate in-hand <span
                    style="color: rgb(224, 216, 120);"><b>object pose</b></span> and
                <span style="color: rgb(137, 37, 139);"><b>extrinsic contact</b></span>. We utilize <i>factor graphs</i>
                to enforce physical constraints and to efficiently
                solve.
            </h2>
        </div>
    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        Prehensile autonomous manipulation, such as peg insertion, tool use, or assembly, require
                        precise
                        in-hand understanding of the object pose and the extrinsic contacts made during
                        interactions. Providing accurate estimation of pose and contacts is challenging. Tactile sensors
                        can
                        provide local geometry at the sensor and force information about the grasp, but the
                        locality of sensing means resolving poses and contacts from tactile alone is often an
                        ill-posed problem, as multiple configurations can be consistent with the observations.
                        Adding visual feedback can help resolve ambiguities, but can suffer from noise and
                        occlusions.
                    </p>
                    <p>
                        In this work, we propose a method that pairs local observations from sensing with the
                        physical constraints of contact. We propose a set of <i>factors</i> that ensure local
                        consistency with
                        tactile observations as well as enforcing physical plausibility, namely, that the estimated
                        pose and contacts must respect the kinematic and force constraints of quasi-static rigid body
                        interactions. We formalize our problem as a <i>factor graph</i>, allowing for efficient
                        estimation.
                        In our experiments, we demonstrate that our method outperforms existing geometric and
                        contact-informed estimation pipelines, especially when only tactile information is available.
                    </p>
                </div>
            </div>
        </div>
        <!--/ Abstract. -->

    </div>
</section>


<section class="section">
    <div class="container is-fluid">

        <!-- Section title -->
        <div class="has-text-centered mb-6">
            <h2 class="title is-3">Method Overview</h2>
        </div>

        <!-- Method overview figure -->
        <div class="mb-6 has-text-centered">
            <figure class="image is-inline-block">
                <img src="./static/images/method_overview.png"
                     alt="TacGraph method overview"
                     style="max-height: 700px; width: 75%; object-fit: contain; display: block; margin: 0 auto;">
            </figure>
        </div>

        <!-- Text description (slightly constrained for readability) -->
        <div class="columns is-centered">
            <div class="column is-three-quarters">
                <div class="content has-text-justified">
                    <p>
                        We propose <b>TacGraph</b> - a method for estimating object pose and extrinsic contacts during
                        prehensile
                        manipulation using distributed tactile feedback and (optionally) visual feedback. Our method has
                        two main
                        components. First, we have a series of <i>learned tactile modules</i>. These translate raw
                        tactile signals
                        to useful intermediates (geometry, forces, and displacements). Second, we have a <i>Factor
                        Graph</i> which
                        takes the learned tactile outputs, along with known object/environment geometries and enforce
                        the physical
                        constraints of contact (geometric consistency, non-penetration, contact kinematics, and force
                        balance).
                    </p>
                </div>
            </div>
        </div>

    </div>
</section>


<section class="section">
    <div class="container is-max-desktop" style="margin-bottom: 20px;">
        <h2 class="title is-3">Results - Tactile Only</h2>
        <div class="container">
            <div id="results-carousel" class="carousel results-carousel" style="margin-bottom: 10px;">
                <div class="item item-screwdriver-1">
                    <video poster="" id="screwdriver-1" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/inference/screwdriver.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-slanted-rectangle-1">
                    <video poster="" id="slanted-rectangle-1" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/inference/slanted_rectangle.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-cylinder-real-1">
                    <video poster="" id="cylinder-real-1" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/inference/cylinder_real.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-wrench-1">
                    <video poster="" id="wrench-1" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/inference/wrench.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-bandu-base-1">
                    <video poster="" id="bandu-base-1" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/inference/bandu_base.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-quarter-cylinder-1">
                    <video poster="" id="quarter-cylinder-1" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/inference/quarter_cylinder.mp4"
                                type="video/mp4">
                    </video>
                </div>
            </div>
            <div class="content">
                <p>
                    We show comparison of <span style="color: rgb(184, 183, 89);">predicted</span> and <span
                        style="color: rgb(83, 123, 42);">ground truth</span>
                    pose and <span style="color: rgb(122, 5, 114);">predicted</span> and <span
                        style="color: rgb(250, 76, 80);">ground truth</span>
                    extrinsic contact estimates from <b>tactile only</b> feedback. The results indicate that the
                    contacts
                    are utilized by our proposed framework to narrow down and disambiguate the local tactile feedback,
                    yielding highly accurate predictions.
                </p>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop">

        <!-- Section header -->
        <h2 class="title is-3">Application: Open-Loop Insertion</h2>

        <!-- 2x2 video grid -->
        <div class="columns is-multiline is-centered">

            <!-- Video 1 -->
            <div class="column is-half">
                <figure>
                    <video autoplay controls muted loop playsinline
                           style="width: 100%; height: auto;">
                        <source src="./static/videos/insertion/tacgraph.mp4"
                                type="video/mp4">
                    </video>
                    <figcaption class="has-text-centered is-size-5 mt-2">
                        (a) <b>TacGraph (Ours)</b>
                    </figcaption>
                </figure>
            </div>

            <!-- Video 2 -->
            <div class="column is-half">
                <figure>
                    <video autoplay controls muted loop playsinline
                           style="width: 100%; height: auto;">
                        <source src="./static/videos/insertion/icp.mp4"
                                type="video/mp4">
                    </video>
                    <figcaption class="has-text-centered is-size-5 mt-2">
                        (b) ICP
                    </figcaption>
                </figure>
            </div>

            <!-- Video 3 -->
            <div class="column is-half">
                <figure>
                    <video autoplay controls muted loop playsinline
                           style="width: 100%; height: auto;">
                        <source src="./static/videos/insertion/chsel.mp4"
                                type="video/mp4">
                    </video>
                    <figcaption class="has-text-centered is-size-5 mt-2">
                        (c) CHSEL
                    </figcaption>
                </figure>
            </div>

            <!-- Video 4 -->
            <div class="column is-half">
                <figure>
                    <video autoplay controls muted loop playsinline
                           style="width: 100%; height: auto;">
                        <source src="./static/videos/insertion/scope.mp4"
                                type="video/mp4">
                    </video>
                    <figcaption class="has-text-centered is-size-5 mt-2">
                        (d) SCOPE (v2)
                    </figcaption>
                </figure>
            </div>

        </div>

        <!-- Text description -->
        <div class="content mt-5">
            <p>
                We demonstrate application of TacGraph for open-loop insertion with tactile-only feedback. We perform
                a series of exploratory pokes and run inference with each method, then use the final estimate to perform
                an open loop insertion with <i>low tolerance</i> (~3mm). We show comparison to several baselines. Our
                method is the only to succeed on this example and has the highest success rate across 4 test objects
                (see paper for details). We show the final estimate of each method against the ground truth and the
                attempted insertion.
            </p>
        </div>

    </div>
</section>

<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@article{vdm2026tacgraph,
  author    = {Van der Merwe, Mark and Ota, Kei and Berenson, Dmitry and Fazeli, Nima and Jha, Devesh},
  title     = {Simultaneous Extrinsic Contact and In-Hand Pose Estimation via Distributed Tactile Sensing},
  journal   = {IEEE Robotics and Automation Letters},
  year      = {2026},
}</code></pre>
    </div>
</section>


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        This website is licensed under a <a rel="license"
                                                            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>
                    <p>
                        This website is based on the <a href="https://nerfies.github.io/">Nerfies</a> website.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
